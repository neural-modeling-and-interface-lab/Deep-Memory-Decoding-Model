{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09c7eaf",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9cd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nexfile # a .py file\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.linalg\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae954072",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03ef67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadnpz(name, allow_pickle=False):\n",
    "    \"\"\"\n",
    "    loadnpz loads compressed files\n",
    "    Args:\n",
    "       name (str): directory of npz file\n",
    "       allow_pickle (bool): argument to allow pickle\n",
    "    Returns:\n",
    "       data (np array): np array from compressed npz file\n",
    "    \"\"\"\n",
    "    if allow_pickle:\n",
    "        data = np.load(name, allow_pickle=True)  # Over-rule default False (loading pickled data can execute arbitrary code)\n",
    "    else:\n",
    "        data = np.load(name)\n",
    "    data = data.f.arr_0  # Gets np array from data, which is currently an instance of class NpzFile, which has f attribute (numpy.lib.npyio.NpzFile)\n",
    "    return data\n",
    "\n",
    "def loadFileNames(data_dir):\n",
    "    \"\"\"\n",
    "    Gives the names of the files in the folder folderName1, which is simply the numeric data folders from /Rodent WFU DNMS\n",
    "    Args:\n",
    "        data_dir (str): local directory that stores the folder for neuron data\n",
    "    Returns:\n",
    "        fileNames (list): list of all .nex filenames as full directory name as str (./Rodent_WFU_DNMS/1193/1193u044merge-clean.nex)\n",
    "    \"\"\"\n",
    "    dir_name =  data_dir + '/driveNeuron' # Local directory as string for Rodent WFU DNMS\n",
    "    folderNames = os.listdir(dir_name) # Get list for directory contents\n",
    "    rat_nums = [] # List to append folders with integer name as rats' id\n",
    "    \n",
    "    # If subdirectory in list is integer then add to list rat_nums\n",
    "    for num in folderNames: \n",
    "        try:\n",
    "            int(num)\n",
    "            rat_nums.append(num)\n",
    "        except:\n",
    "            True    \n",
    "    \n",
    "    fileNames = [] # the full directory of each event for all the rats\n",
    "    \n",
    "    # Generate list of all WFU Rat DMS file names with full directory name\n",
    "    for num in rat_nums: \n",
    "        folder = dir_name + \"/\" + num + \"/\" # folder directory for each rat\n",
    "        events = os.listdir(folder) # list of one rat's .nex file name for each event\n",
    "        for a in range(len(events)): \n",
    "            events[a] = folder + events[a] # concatenate the full directory name for each event .nex file\n",
    "        fileNames = fileNames + events \n",
    "\n",
    "    return fileNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fbb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***Change this according to local directory***\n",
    "data_dir = '/Volumes/TOSHIBA/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b1b0b",
   "metadata": {},
   "source": [
    "## Event Data Preprocessing (save data to .npz files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02dc46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveEventData(data_dir, eventNames, fileLabel):\n",
    "    \"\"\"\n",
    "    This function saves: \n",
    "    the spike trains around the events, \n",
    "    input names (the wire/cell names of each input neuron spike channel), \n",
    "    the index of the event being predicted,\n",
    "    the index label of valid sessions\n",
    "    Args:\n",
    "       data_dir (str): local directory that stores the folder for neuron data\n",
    "       eventNames (list of str): names of events\n",
    "       fileLabel (str): label for saved files\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    \n",
    "    #\"validArgs\" will be the set of sessions with valid data that can be read with nexfile reader.\n",
    "    validArgs = []\n",
    "\n",
    "    #This gives the file names of the sessions.\n",
    "    fileNames = loadFileNames(data_dir)\n",
    "        \n",
    "    # loop over all the sessions (of all the rats), total 519 sessions\n",
    "    for a in range(len(fileNames)): \n",
    "        fileName = fileNames[a] # full directory of one file\n",
    "        folderName1 = fileName.split('/')[3] # folder name, int id num of rat\n",
    "        \n",
    "        #There is an issue with the data in the folder labeled 1127, that causes it to be different from all the other sessions.\n",
    "        if folderName1 != '1127':\n",
    "            try:\n",
    "                #This loads the data\n",
    "                reader = nexfile.Reader()\n",
    "                fileData = reader.ReadNexFile(fileName)\n",
    "                validArgs.append(a)\n",
    "            except:\n",
    "                True\n",
    "\n",
    "            #This gets the names of all the variables which have timestamps data\n",
    "            variableNames = []\n",
    "            for b in range(len(fileData['Variables'])):\n",
    "                if 'Timestamps' in fileData['Variables'][b].keys():\n",
    "                    varName = fileData['Variables'][b]['Header'][\"Name\"]\n",
    "                    variableNames.append(varName)                  \n",
    "            variableNames = np.array(variableNames)\n",
    "                        \n",
    "            #This gets the neuron spike data and the event time data.\n",
    "            inputNums = []\n",
    "            inputNames = []\n",
    "            outputNums = []\n",
    "            outputNames = []\n",
    "            timeData = []\n",
    "            b = 0\n",
    "            for b0 in range(len(fileData['Variables'])):\n",
    "                if 'Timestamps' in fileData['Variables'][b].keys():\n",
    "                    #This gives the timing data for the variable\n",
    "                    times1 = fileData['Variables'][b]['Timestamps']\n",
    "                    times1 = np.array(times1)\n",
    "                    timeData.append(np.copy(times1))\n",
    "\n",
    "                    #This puts the variable number in \"spikeData\" for each relevent variable\n",
    "                    name = variableNames[b]\n",
    "                    #This appends the arg in \"fileData['Variables']\" which corresponds to the variable \"name\"\n",
    "                    if ('wire' in name) and ('cell' in name):\n",
    "                        inputNums.append(b) \n",
    "                        inputNames.append(name)\n",
    "                        \n",
    "                    #This appends the arg in \"fileData['Variables']\" which corresponds to the variable \"name\"\n",
    "                    if name in events:\n",
    "                        outputNums.append(b) \n",
    "                        outputNames.append(name)\n",
    "                        \n",
    "                    b += 1\n",
    "            outputNums = np.array(outputNums)[np.argsort(np.array(outputNames))]\n",
    "            \n",
    "            #outputType will give the number of the event in \"events\" for each time of each event.\n",
    "            numOutput = 0 \n",
    "            outputType = np.array([])\n",
    "            \n",
    "            for b0 in range(len(outputNums)):\n",
    "                b = outputNums[b0]\n",
    "                times0 = timeData[b] #This gives the timing of events.\n",
    "                numOutput += times0.shape[0]\n",
    "\n",
    "                b1 = np.argwhere(events == outputNames[b0])[0, 0] #This gives the argument in \"eventss\" of this variable.\n",
    "\n",
    "                outputType = np.concatenate(( outputType, np.zeros(times0.shape) + b1  ))\n",
    "\n",
    "            #spikeTimer will include the spike trains in the 10 second interval around each event.\n",
    "            #spikeTimer = np.zeros((numOutput, len(inputNums), 2000 ))\n",
    "            spikeTimer = np.zeros((numOutput, len(inputNums), 5000 ))          \n",
    "            b0 = 0\n",
    "            for b in outputNums: #Iterating through output events\n",
    "                times0 = timeData[b] #This is the times of this event\n",
    "                for c in range(times0.shape[0]):\n",
    "                    timeNow = times0[c] #This is a particular time of a particular event\n",
    "                    d0 = 0\n",
    "                    for d in inputNums:\n",
    "                        spikes = timeData[d] - timeNow #This gives the timing of neuron spikes relative to the event.\n",
    "                        spikes = spikes[np.abs(spikes) < 5] #This gives only neuron spikes within 5 seconds of the event.\n",
    "                        spikes = spikes + 5 #This gives the timing of these neuron spikes relative to a 10 second window around the event\n",
    "                        spikes = np.floor(spikes * 500).astype(int) #This rounds the time to the nearest 500th of a second.\n",
    "                        spikeTimer[b0, d0, spikes] = 1 #This converts the spike times to a binary spike train, and adds it to the full spike train array.\n",
    "                        d0 += 1\n",
    "                    b0 += 1\n",
    "        \n",
    "            np.savez_compressed(data_dir+'/eventData/seperate/data_' + fileLabel + '_' + str(a) + '.npz', spikeTimer) #This saves the neuron spike data \n",
    "            inputNames = np.array(inputNames)\n",
    "            np.savez_compressed(data_dir+'/eventData/seperate/input_' + fileLabel + '_' + str(a) + '.npz', inputNames) #This saves the wire/cell names of each input neuron spike channel.\n",
    "            np.savez_compressed(data_dir+'/eventData/seperate/output_' + fileLabel + '_' + str(a) + '.npz', outputType) #This saves the index of the event being predicted\n",
    "\n",
    "    validArgs = np.array(validArgs)\n",
    "    np.savez_compressed(data_dir+'/eventData/seperate/validArgs_' + fileLabel + '.npz', validArgs) #This saves which sessions have valid data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158a8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineEventData(data_dir, eventNames, fileLabel):\n",
    "    \"\"\"\n",
    "    This function saves: \n",
    "        the neuron spike data,\n",
    "        the index of the events,\n",
    "        the valid session index,\n",
    "        the binary of which neurons exist in each session\n",
    "    Args:\n",
    "       data_dir (str): local directory that stores the folder for neuron data\n",
    "       eventNames (list of str): names of events\n",
    "       fileLabel (str): label for saved files\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "\n",
    "    M = 50\n",
    "    #M = 25\n",
    "\n",
    "    fileNames = loadFileNames(data_dir)\n",
    "\n",
    "    validArgs = loadnpz(data_dir + '/eventData/seperate/validArgs_' + fileLabel + '.npz')\n",
    "    inputNamesAll = np.array([])\n",
    "    outputTypeAll = np.array([])\n",
    "    keyAll = np.array([])\n",
    "    for a0 in range(len(validArgs)):\n",
    "        a = validArgs[a0]\n",
    "        inputNames = loadnpz(data_dir + '/eventData/seperate/input_' + fileLabel + '_' + str(a) + '.npz')\n",
    "        outputType = loadnpz(data_dir + '/eventData/seperate/output_' + fileLabel + '_' + str(a) + '.npz')\n",
    "\n",
    "        #print (np.unique(outputType))\n",
    "\n",
    "        #print (fileNames[a])\n",
    "        #print (inputNames)\n",
    "\n",
    "        #This loop removes '_ver_0' from names\n",
    "        for b in range(len(inputNames)):\n",
    "            if inputNames[b][-len('_ver_0'):] == '_ver_0':\n",
    "                inputNames[b] = inputNames[b][:-len('_ver_0')]\n",
    "\n",
    "        #This combines the data from sessions\n",
    "        inputNamesAll = np.concatenate((inputNamesAll, inputNames))\n",
    "        outputTypeAll = np.concatenate((outputTypeAll, outputType))\n",
    "        keyAll = np.concatenate((keyAll, np.zeros(outputType.shape[0]) + a0  ))\n",
    "\n",
    "    keyAll = keyAll.astype(int)\n",
    "\n",
    "    inputNamesUnique = np.unique(inputNamesAll) #This is a list of unique neuron channel names\n",
    "\n",
    "    np.savez_compressed(data_dir + '/eventData/general/uniqueNames.npz', inputNamesUnique)\n",
    "\n",
    "\n",
    "\n",
    "    #This array will contain all the combined neuron spike data.\n",
    "    dataAll = np.zeros((outputTypeAll.shape[0], inputNamesUnique.shape[0], 100 ))\n",
    "    #dataAll = np.zeros((outputTypeAll.shape[0], inputNamesUnique.shape[0], 200 ))\n",
    "\n",
    "    #sensorLocation is a binary array showing which neuron channels exist in this session\n",
    "    sensorLocation = np.zeros(( validArgs.shape[0], inputNamesUnique.shape[0] ))\n",
    "    count1 = 0\n",
    "    for a0 in range(len(validArgs)):\n",
    "        a = validArgs[a0]\n",
    "        \n",
    "        print(a0 , '/' , len(validArgs))\n",
    "\n",
    "        inputNames = loadnpz(data_dir + '/eventData/seperate/input_' + fileLabel + '_' + str(a) + '.npz')\n",
    "        inputArgs = []\n",
    "        for b in range(len(inputNames)):\n",
    "            if inputNames[b][-len('_ver_0'):] == '_ver_0': #Removing \"ver_0\" from name\n",
    "                inputNames[b] = inputNames[b][:-len('_ver_0')] #Removing \"ver_0\" from name\n",
    "            #This finds the number corresponding to the neuron channel name of inputNames[b]\n",
    "            arg1 = np.argwhere(inputNamesUnique == inputNames[b])[0, 0]\n",
    "            inputArgs.append(arg1)\n",
    "        inputArgs = np.array(inputArgs).astype(int) #inputArgs is the arguments of the subset of inputNamesUnique which is equal to 'inputNames'\n",
    "\n",
    "\n",
    "        sensorLocation[a0, inputArgs] = 1\n",
    "\n",
    "        data = loadnpz(data_dir + '/eventData/seperate/data_' + fileLabel + '_' + str(a) + '.npz')\n",
    "        data = data.reshape((data.shape[0], data.shape[1], data.shape[2] // M, M ))\n",
    "        data = np.sum(data, axis=3) #This modifies the timing to measure how many spikes have occured in a time period of M/500 seconds. For M = 50, it is one 10th of a second.\n",
    "\n",
    "        shape1 = data.shape\n",
    "        #data1 = data.reshape((shape1[0]*shape1[1]*shape1[2],))\n",
    "        #plt.hist(data1, bins=100)\n",
    "        #plt.show()\n",
    "        #quit()\n",
    "\n",
    "        #data[data > 1] = 1\n",
    "\n",
    "        #data[data > 2] = 2\n",
    "        #data = data / 2\n",
    "\n",
    "        #data[data > 5] = 5\n",
    "        #data = data / 3\n",
    "\n",
    "        #data[data > 3] = 3\n",
    "        #data = data / 3\n",
    "\n",
    "        data = np.log(data + 1) #This is a numerical transformation of the number of spikes which occur. This transformation prevents the values in \"data\" from being overly large in cases where many spikes occur rapidly.\n",
    "        size1 = data.shape[0]\n",
    "\n",
    "        #plt.plot(np.sum(np.sum(data, axis=1), axis=1))\n",
    "        #plt.show()\n",
    "\n",
    "        dataAll[count1:count1+size1, inputArgs] = np.copy(data) #This adds the spike data to the array of all spike data.\n",
    "\n",
    "        count1 += size1\n",
    "\n",
    "\n",
    "    #quit()\n",
    "\n",
    "    #plt.plot(np.sum(np.sum(dataAll, axis=1), axis=1))\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    np.savez_compressed(data_dir + '/eventData/combined/data_' + fileLabel + '.npz', dataAll ) #This saves the neuron spike data\n",
    "    np.savez_compressed(data_dir + '/eventData/combined/outputType_' + fileLabel + '.npz', outputTypeAll ) #This saves the event labels\n",
    "    np.savez_compressed(data_dir + '/eventData/combined/keys_' + fileLabel + '.npz', keyAll ) #This saves the session numbers\n",
    "    np.savez_compressed(data_dir + '/eventData/combined/inputLocation_' + fileLabel + '.npz', sensorLocation ) #This saves the binary of which neurons exist in each session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf63f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"includeName\" specifies which events are being predicted. It saves the neuron spike trains around these events.\n",
    "# 'ALL_A_NM_M' 'ALL_BPs' 'ALL_B_NM_M' 'ALL_NM_PHASE' 'ALL_S_PHASE'\n",
    "# 'A_MATCH' 'A_NONMATCH' 'A_SAMPLES' 'AllFile' 'B_MATCH' 'B_NONMATCH'\n",
    "# 'B_SAMPLES' 'DELAY' 'FAILURE' 'LASTNP' 'NOSEPOKE' 'NP' 'OFFER2' 'OFFERA'\n",
    "# 'OFFERB' 'RETRACT' 'REWARDCOUNT' 'STARTSYNC' 'STOPSYNC' 'SUCCESS' 'TRIAL'\n",
    "# eventNames = ['A_MATCH', 'A_NONMATCH', 'B_MATCH', 'B_NONMATCH', 'A_SAMPLES', 'B_SAMPLES']\n",
    "# eventNames = np.array(eventNames)\n",
    "\n",
    "# fileLabel_dict = {\"AM&S\": [0,4], \"ANM&S\": [1,4], \"BM&S\": [2,5], \"BNM&S\": [3,5], \"AM&NM&S\": [0,1,4], \"BM&NM&S\": [2,3,5]}\n",
    "\n",
    "# for i in fileLabel_dict:\n",
    "#     print(i, fileLabel_dict[i]) # i = fileLabel\n",
    "#     print(eventNames[fileLabel_dict[i]]) # eventNames\n",
    "    \n",
    "#     saveEventData(data_dir, eventNames[fileLabel_dict[i]], i)\n",
    "# #     investigateEventData(data_dir, eventNames[fileLabel_dict[i]], i)\n",
    "#     combineEventData(data_dir, eventNames[fileLabel_dict[i]], i)\n",
    "#     print(\"Complete \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01000553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a49bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_events(outputNames, outputNums, timeData, new_events, included_events):\n",
    "    '''\n",
    "    This function sort the included_events events by timestamp and extract the events that are valid (Sample event followed by Match or Nonmatch event) \n",
    "    Args:\n",
    "        outputNames (list of str): the name of events used as output\n",
    "        outputNums (list of int): the index of events for output in timeData\n",
    "        timeData: the Timestamps of all variables\n",
    "        new_events (list of str): ['A_S|M', 'A_S|NM'] or ['B_S|M', 'B_S|NM'] (Match must be before NM events) ***coding structure needs fixing\n",
    "        included_events: (list of str) events needed to find the specific event given the other event happened (All A or B events)\n",
    "    Returns:\n",
    "        updated timeData, outputNums, outputNames\n",
    "    '''\n",
    "    # find the index of included_events in outputNames & outputNums\n",
    "    for i in included_events:\n",
    "        if \"_MATCH\" in i:\n",
    "            if i in outputNames: # check if i is outputNames\n",
    "                M_idx = outputNames.index(i)\n",
    "                _M = timeData[outputNums[M_idx]] # '_MATCH' timestamps\n",
    "                _M = [(j, i) for j in _M]\n",
    "            else: # if not, the list of timestamps is empty\n",
    "                _M = [] \n",
    "                \n",
    "        if \"_NON\" in i:\n",
    "            if i in outputNames:\n",
    "                NM_idx = outputNames.index(i)\n",
    "                _NM = timeData[outputNums[NM_idx]] # '_NONMATCH' timestamps\n",
    "                _NM = [(j, i) for j in _NM]\n",
    "            else:\n",
    "                _NM = []\n",
    "            \n",
    "        if \"_SAMPLE\" in i:\n",
    "            if i in outputNames:\n",
    "                S_idx = outputNames.index(i)        \n",
    "                _S = timeData[outputNums[S_idx]] # '_SAMPLES' timestamps\n",
    "                _S = [(j, i) for j in _S]\n",
    "            else:\n",
    "                _S = []\n",
    "    \n",
    "    _all = sorted(_S + _NM + _M) # sort all events of included events (one position of A or B) \n",
    "#     print('_S', len(_S))\n",
    "#     print('_M', len(_M))\n",
    "#     print('_NM', len(_NM))\n",
    "#     print('_all', len(_all))\n",
    "#     print(_all)\n",
    "    \n",
    "    # to store the index of events timestamps\n",
    "    t_idx_SM = [] \n",
    "    t_idx_SNM = []\n",
    "    t_idx_M = []\n",
    "    t_idx_NM = []\n",
    "    # loop over all the ordered timestamps \n",
    "    for i in np.arange(0, len(_all)-1):\n",
    "        # record the timestamp index of Sample given Match event and the following Match event\n",
    "        if '_SAMPLES' in _all[i][1] and '_MATCH' in _all[i+1][1]: \n",
    "            t_idx_SM.append(i)\n",
    "            t_idx_M.append(i+1)\n",
    "        # record the timestamp index of Sample given NM event and the following NM event\n",
    "        if '_SAMPLES' in _all[i][1] and '_NONMATCH' in _all[i+1][1]: \n",
    "            t_idx_SNM.append(i)\n",
    "            t_idx_NM.append(i+1)\n",
    "    \n",
    "    S_M_timestamps = np.array([_all[i][0] for i in t_idx_SM]) # stores the Timestamps of Sample given Match events\n",
    "    S_NM_timestamps = np.array([_all[i][0] for i in t_idx_SNM]) # stores the Timestamps of Sample given NM events   \n",
    "    M_timestamps = np.array([_all[i][0] for i in t_idx_M]) # stores the Timestamps of Match events\n",
    "    NM_timestamps = np.array([_all[i][0] for i in t_idx_NM]) # stores the Timestamps of NM events\n",
    "    \n",
    "    # if the events are not in correct sequence, update the timeData with corrected timestamps\n",
    "    if (len(S_M_timestamps) != len(_M)) or (len(S_NM_timestamps) != len(_NM)):\n",
    "#         print('Target Timestamps SM', len(S_M_timestamps))\n",
    "#         print('Target Timestamps SNM', len(S_NM_timestamps))\n",
    "#         print('Target Timestamps M', len(M_timestamps))\n",
    "#         print('Target Timestamps NM', len(NM_timestamps))\n",
    "#         print('_M', len(_M))\n",
    "#         print('_NM', len(_NM))\n",
    "\n",
    "#         print(len(timeData[outputNums[M_idx]]), len(M_timestamps))\n",
    "        timeData[outputNums[M_idx]] = M_timestamps\n",
    "\n",
    "#         print(len(timeData[outputNums[NM_idx]]), len(NM_timestamps))\n",
    "        timeData[outputNums[NM_idx]] = NM_timestamps\n",
    "#         print(timeData[outputNums[NM_idx]])\n",
    "#         print(S_NM_timestamps)\n",
    "\n",
    "    # append the filtered sample timestamps at the end of the timeData\n",
    "    timeData.append(S_M_timestamps)\n",
    "    # append the index of timeData to outputNums\n",
    "    outputNums = np.append(outputNums, len(timeData)-1)\n",
    "\n",
    "    timeData.append(S_NM_timestamps)\n",
    "    outputNums = np.append(outputNums, len(timeData)-1)\n",
    "    \n",
    "    for i in new_events:\n",
    "        outputNames.append(i)\n",
    "    \n",
    "    return timeData, outputNums, outputNames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "63163bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveEventData2(data_dir, eventNames, fileLabel, rm_groups = []):\n",
    "    \"\"\"\n",
    "    This function saves: \n",
    "        the spike trains around the events, \n",
    "        input names (the wire/cell names of each input neuron spike channel), \n",
    "        the index of the event being predicted,\n",
    "        the index label of valid sessions\n",
    "        the names of output events \n",
    "    Args:\n",
    "       data_dir (str): local directory that stores the folder for neuron data\n",
    "       eventNames (list of str): names of all events\n",
    "       fileLabel (str): label for saving files\n",
    "       rm_groups (list of str): names of events to be removed from eventNames to form outputNames\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "\n",
    "#     output = []\n",
    "    \n",
    "    #\"validArgs\" will be the set of sessions with valid data that can be read with nexfile reader.\n",
    "    validArgs = []\n",
    "\n",
    "    #This gives the file names of the sessions.\n",
    "    fileNames = loadFileNames(data_dir)\n",
    "    \n",
    "    print(fileLabel)\n",
    "    \n",
    "    # loop over all the sessions (of all the rats), total 519 sessions\n",
    "    for a in np.arange(len(fileNames)): \n",
    "        fileName = fileNames[a] # full directory of one file\n",
    "        folderName1 = fileName.split('/')[3] # folder name, int id num of rat\n",
    "        \n",
    "        #There is an issue with the data in the folder labeled 1127, that causes it to be different from all the other sessions.\n",
    "        if folderName1 != '1127':\n",
    "            try:\n",
    "                #This loads the data\n",
    "                reader = nexfile.Reader()\n",
    "                fileData = reader.ReadNexFile(fileName)\n",
    "                validArgs.append(a)\n",
    "            except:\n",
    "                True\n",
    "\n",
    "            #This gets the names of all the variables which have timestamps data\n",
    "            variableNames = []\n",
    "            for b in range(len(fileData['Variables'])):\n",
    "                if 'Timestamps' in fileData['Variables'][b].keys():\n",
    "                    varName = fileData['Variables'][b]['Header'][\"Name\"]\n",
    "                    variableNames.append(varName)                  \n",
    "            variableNames = np.array(variableNames)\n",
    "            \n",
    "                        \n",
    "            # This gets the neuron spike data and the event time data.\n",
    "            inputNums = [] # index of inputs in timeData\n",
    "            inputNames = [] # input names\n",
    "            outputNums = [] # index of output events in timeData (same size ad outputNames)\n",
    "            outputNames = [] # output event names\n",
    "            timeData = [] # list of Timestamps for each variable with Timestamps\n",
    "            b = 0 # to record index of timeData (keep count of timeData)\n",
    "            for b0 in np.arange(len(fileData['Variables'])):\n",
    "                if 'Timestamps' in fileData['Variables'][b0].keys():\n",
    "                    #This gives the timestamps for all the variables with 'Timestamps'\n",
    "                    times = fileData['Variables'][b0]['Timestamps']\n",
    "                    times = np.array(times)\n",
    "                    timeData.append(np.copy(times))\n",
    "\n",
    "                    #This puts the variable number in \"spikeData\" for each relevent variable\n",
    "                    name = fileData['Variables'][b0]['Header'][\"Name\"]\n",
    "\n",
    "                    #This appends the arg in \"fileData['Variables']\" which corresponds to the neuron signal input\n",
    "                    if ('wire' in name) and ('cell' in name):\n",
    "                        inputNums.append(b) \n",
    "                        inputNames.append(name)\n",
    "\n",
    "                    #This appends the arg in \"fileData['Variables']\" which corresponds to the output events\n",
    "                    if name in eventNames:\n",
    "                        outputNums.append(b) \n",
    "                        outputNames.append(name)\n",
    "\n",
    "                    b+=1\n",
    "\n",
    "            outputNums = np.array(outputNums)[np.argsort(np.array(outputNames))]\n",
    "            \n",
    "            # adding new groups of Sample events given conditions\n",
    "            timeData, outputNums, outputNames = add_events(outputNames, outputNums, timeData, new_events = ['A_S|M', 'A_S|NM'], included_events = ['A_MATCH', 'A_NONMATCH', 'A_SAMPLES'])\n",
    "            timeData, outputNums, outputNames = add_events(outputNames, outputNums, timeData, new_events = ['B_S|M', 'B_S|NM'], included_events = ['B_MATCH', 'B_NONMATCH', 'B_SAMPLES'])\n",
    "#             print(\"outputNames\", outputNames)\n",
    "#             print(\"outputNums\", outputNums)\n",
    "#             print(\"timeData\", len(timeData))\n",
    "            \n",
    "            # remove the events that are not needed \n",
    "            if len(rm_groups)!= 0:\n",
    "                for e in rm_groups:\n",
    "                    idx = outputNames.index(e)\n",
    "                    outputNames.pop(idx)\n",
    "                    outputNums = np.delete(outputNums, idx)\n",
    "\n",
    "            #outputType will give the number of the event in \"events\" for each time of each event.\n",
    "            numOutput = 0 \n",
    "            outputType = np.array([]) # index of each output events in eventNames\n",
    "\n",
    "            for b0 in np.arange(len(outputNums)):\n",
    "                times = timeData[outputNums[b0]] # This gives the Timestamps of an output event.\n",
    "                numOutput += times.shape[0]\n",
    "\n",
    "#                 events_idx = eventNames.index(outputNames[b0]) # This gives the argument in \"events\" of this variable.\n",
    "#                 print(eventNames)\n",
    "#                 print(outputNames[b0])\n",
    "                events_idx = b0\n",
    "                outputType = np.concatenate((outputType, np.zeros(times.shape) + events_idx))\n",
    "#             print(\"outputType\", outputType)\n",
    "\n",
    "            # spikeTimer will include the spike trains in the 10 second interval around each event.\n",
    "            #spikeTimer = np.zeros((numOutput, len(inputNums), 2000 ))\n",
    "            spikeTimer = np.zeros((numOutput, len(inputNums), 5000 ))          \n",
    "            b0 = 0\n",
    "            for b in outputNums: #Iterating through output events\n",
    "                times0 = timeData[b] #This is the times of this event\n",
    "                for c in range(times0.shape[0]):\n",
    "                    timeNow = times0[c] #This is a particular time of a particular event\n",
    "                    d0 = 0\n",
    "                    for d in inputNums:\n",
    "                        spikes = timeData[d] - timeNow #This gives the timing of neuron spikes relative to the event.\n",
    "                        spikes = spikes[np.abs(spikes) < 5] #This gives only neuron spikes within 5 seconds of the event.\n",
    "                        spikes = spikes + 5 #This gives the timing of these neuron spikes relative to a 10 second window around the event\n",
    "                        spikes = np.floor(spikes * 500).astype(int) #This rounds the time to the nearest 500th of a second.\n",
    "                        spikeTimer[b0, d0, spikes] = 1 #This converts the spike times to a binary spike train, and adds it to the full spike train array.\n",
    "                        d0 += 1\n",
    "                    b0 += 1\n",
    "                    \n",
    "            np.savez_compressed(data_dir+'/eventData/seperate/data_' + fileLabel + '_' + str(a) + '.npz', spikeTimer) #This saves the neuron spike data (8--8th version of minor modification)\n",
    "            inputNames = np.array(inputNames)\n",
    "            np.savez_compressed(data_dir+'/eventData/seperate/input_' + fileLabel + '_' + str(a) + '.npz', inputNames) #This saves the wire/cell names of each input neuron spike channel.\n",
    "            np.savez_compressed(data_dir+'/eventData/seperate/output_' + fileLabel + '_' + str(a) + '.npz', outputType) #This saves the number of the type of event being predicted\n",
    "            print(\"Saved \" + fileLabel + '_' + str(a))\n",
    "\n",
    "#     # to check the number of each saved events         \n",
    "#     output = np.concatenate((output, outputType))\n",
    "#     a, c = np.unique(output, return_counts=True)\n",
    "#     for i in np.arange(len(a)):\n",
    "#     print(outputNames[i], c[i])\n",
    "        \n",
    "    validArgs = np.array(validArgs)\n",
    "    np.savez_compressed(data_dir+'/eventData/seperate/validArgs_' + fileLabel + '.npz', validArgs) #This saves which sessions have valid data.\n",
    "    np.savez_compressed(data_dir+'/eventData/seperate/outputNames_' + fileLabel + '.npz', outputNames) # This saves the outputNames for reference of the index in output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7964bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventNames = ['A_MATCH', 'A_NONMATCH', 'A_SAMPLES', 'B_MATCH', 'B_NONMATCH', 'B_SAMPLES', 'A_S|M', 'A_S|NM', 'B_S|M', 'B_S|NM']\n",
    "fileLabel = \"6\" # 'S|M & S|NM'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f5ad2add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Saved 6_0\n",
      "Saved 6_1\n",
      "Saved 6_2\n",
      "Saved 6_3\n",
      "Saved 6_4\n",
      "Saved 6_5\n",
      "Saved 6_6\n",
      "Saved 6_7\n",
      "Saved 6_8\n",
      "Saved 6_9\n",
      "Saved 6_10\n",
      "Saved 6_11\n",
      "Saved 6_12\n",
      "Saved 6_13\n",
      "Saved 6_14\n",
      "Saved 6_15\n",
      "Saved 6_16\n",
      "Saved 6_17\n",
      "Saved 6_18\n",
      "Saved 6_19\n",
      "4 4\n",
      "40 37\n",
      "11 11\n",
      "25 24\n",
      "Saved 6_20\n",
      "Saved 6_21\n",
      "Saved 6_22\n",
      "Saved 6_23\n",
      "Saved 6_24\n",
      "Saved 6_25\n",
      "Saved 6_26\n",
      "Saved 6_27\n",
      "Saved 6_28\n",
      "Saved 6_29\n",
      "Saved 6_30\n",
      "Saved 6_31\n",
      "Saved 6_32\n",
      "Saved 6_33\n",
      "Saved 6_34\n",
      "Saved 6_35\n",
      "Saved 6_36\n",
      "Saved 6_37\n",
      "Saved 6_38\n",
      "Saved 6_39\n",
      "Saved 6_40\n",
      "Saved 6_41\n",
      "Saved 6_42\n",
      "Saved 6_43\n",
      "Saved 6_44\n",
      "Saved 6_45\n",
      "Saved 6_46\n",
      "Saved 6_47\n",
      "Saved 6_48\n",
      "Saved 6_49\n",
      "Saved 6_50\n",
      "Saved 6_51\n",
      "Saved 6_52\n",
      "Saved 6_53\n",
      "Saved 6_54\n",
      "Saved 6_55\n",
      "Saved 6_56\n",
      "Saved 6_57\n",
      "Saved 6_58\n",
      "Saved 6_59\n",
      "Saved 6_60\n",
      "Saved 6_61\n",
      "Saved 6_62\n",
      "Saved 6_63\n",
      "Saved 6_64\n",
      "Saved 6_65\n",
      "22 19\n",
      "24 23\n",
      "Saved 6_66\n",
      "Saved 6_67\n",
      "43 38\n",
      "6 6\n",
      "Saved 6_68\n",
      "Saved 6_69\n",
      "Saved 6_70\n",
      "Saved 6_71\n",
      "35 31\n",
      "14 13\n",
      "5 4\n",
      "26 25\n",
      "Saved 6_72\n",
      "Saved 6_73\n",
      "Saved 6_74\n",
      "Saved 6_75\n",
      "Saved 6_76\n",
      "Saved 6_77\n",
      "Saved 6_78\n",
      "Saved 6_79\n",
      "Saved 6_80\n",
      "Saved 6_81\n",
      "Saved 6_82\n",
      "Saved 6_83\n",
      "Saved 6_84\n",
      "Saved 6_85\n",
      "Saved 6_86\n",
      "Saved 6_87\n",
      "Saved 6_88\n",
      "Saved 6_89\n",
      "15 13\n",
      "65 63\n",
      "Saved 6_90\n",
      "Saved 6_91\n",
      "Saved 6_92\n",
      "Saved 6_93\n",
      "Saved 6_94\n",
      "Saved 6_95\n",
      "Saved 6_96\n",
      "7 7\n",
      "44 43\n",
      "5 5\n",
      "24 23\n",
      "Saved 6_97\n",
      "Saved 6_98\n",
      "Saved 6_99\n",
      "Saved 6_100\n",
      "Saved 6_101\n",
      "Saved 6_102\n",
      "Saved 6_103\n",
      "Saved 6_104\n",
      "10 5\n",
      "30 21\n",
      "9 7\n",
      "27 19\n",
      "Saved 6_105\n",
      "Saved 6_106\n",
      "Saved 6_107\n",
      "Saved 6_108\n",
      "Saved 6_109\n",
      "Saved 6_110\n",
      "9 9\n",
      "23 22\n",
      "Saved 6_111\n",
      "Saved 6_112\n",
      "Saved 6_113\n",
      "Saved 6_114\n",
      "Saved 6_115\n",
      "Saved 6_116\n",
      "Saved 6_117\n",
      "Saved 6_118\n",
      "Saved 6_119\n",
      "Saved 6_120\n",
      "Saved 6_121\n",
      "Saved 6_122\n",
      "Saved 6_123\n",
      "Saved 6_124\n",
      "Saved 6_125\n",
      "7 7\n",
      "34 33\n",
      "Saved 6_126\n",
      "Saved 6_127\n",
      "Saved 6_128\n",
      "Saved 6_129\n",
      "Saved 6_130\n",
      "Saved 6_131\n",
      "Saved 6_132\n",
      "Saved 6_133\n",
      "Saved 6_134\n",
      "Saved 6_135\n",
      "Saved 6_136\n",
      "17 16\n",
      "31 31\n",
      "19 16\n",
      "33 31\n",
      "Saved 6_137\n",
      "Saved 6_138\n",
      "Saved 6_139\n",
      "Saved 6_140\n",
      "Saved 6_141\n",
      "Saved 6_142\n",
      "Saved 6_143\n",
      "Saved 6_144\n",
      "11 10\n",
      "27 25\n",
      "5 4\n",
      "37 34\n",
      "Saved 6_145\n",
      "Saved 6_146\n",
      "Saved 6_147\n",
      "Saved 6_148\n",
      "Saved 6_149\n",
      "Saved 6_150\n",
      "Saved 6_151\n",
      "Saved 6_152\n",
      "Saved 6_153\n",
      "Saved 6_154\n",
      "Saved 6_155\n",
      "Saved 6_156\n",
      "Saved 6_157\n",
      "Saved 6_158\n",
      "Saved 6_159\n",
      "Saved 6_160\n",
      "Saved 6_161\n",
      "Saved 6_162\n",
      "4 3\n",
      "49 49\n",
      "6 6\n",
      "41 40\n",
      "Saved 6_163\n",
      "Saved 6_164\n",
      "Saved 6_165\n",
      "Saved 6_166\n",
      "Saved 6_167\n",
      "Saved 6_168\n",
      "Saved 6_169\n",
      "Saved 6_170\n",
      "Saved 6_171\n",
      "Saved 6_172\n",
      "Saved 6_173\n",
      "Saved 6_174\n",
      "Saved 6_175\n",
      "Saved 6_176\n",
      "Saved 6_177\n",
      "Saved 6_178\n",
      "Saved 6_179\n",
      "Saved 6_180\n",
      "Saved 6_181\n",
      "Saved 6_182\n",
      "Saved 6_183\n",
      "Saved 6_184\n",
      "Saved 6_185\n",
      "Saved 6_186\n",
      "Saved 6_187\n",
      "Saved 6_188\n",
      "Saved 6_189\n",
      "Saved 6_190\n",
      "Saved 6_191\n",
      "Saved 6_192\n",
      "Saved 6_193\n",
      "Saved 6_194\n",
      "Saved 6_195\n",
      "Saved 6_196\n",
      "Saved 6_197\n",
      "Saved 6_198\n",
      "Saved 6_199\n",
      "6 6\n",
      "18 17\n",
      "Saved 6_200\n",
      "Saved 6_201\n",
      "Saved 6_202\n",
      "Saved 6_203\n",
      "Saved 6_204\n",
      "Saved 6_205\n",
      "Saved 6_206\n",
      "Saved 6_207\n",
      "Saved 6_208\n",
      "Saved 6_209\n",
      "Saved 6_210\n",
      "Saved 6_211\n",
      "Saved 6_212\n",
      "Saved 6_213\n",
      "Saved 6_214\n",
      "4 3\n",
      "30 28\n",
      "15 12\n",
      "31 28\n",
      "Saved 6_215\n",
      "Saved 6_216\n",
      "Saved 6_217\n",
      "Saved 6_218\n",
      "Saved 6_219\n",
      "Saved 6_220\n",
      "Saved 6_221\n",
      "Saved 6_222\n",
      "17 13\n",
      "24 22\n",
      "8 6\n",
      "31 27\n",
      "Saved 6_223\n",
      "Saved 6_224\n",
      "Saved 6_225\n",
      "7 6\n",
      "32 26\n",
      "9 6\n",
      "32 23\n",
      "Saved 6_226\n",
      "Saved 6_227\n",
      "Saved 6_228\n",
      "Saved 6_229\n",
      "Saved 6_230\n",
      "Saved 6_231\n",
      "Saved 6_232\n",
      "Saved 6_233\n",
      "Saved 6_234\n",
      "Saved 6_235\n",
      "Saved 6_236\n",
      "Saved 6_237\n",
      "Saved 6_238\n",
      "Saved 6_239\n",
      "Saved 6_240\n",
      "Saved 6_241\n",
      "Saved 6_242\n",
      "Saved 6_243\n",
      "Saved 6_244\n",
      "Saved 6_245\n",
      "Saved 6_246\n",
      "Saved 6_247\n",
      "Saved 6_248\n",
      "Saved 6_249\n",
      "4 4\n",
      "26 24\n",
      "10 5\n",
      "22 22\n",
      "Saved 6_250\n",
      "8 8\n",
      "30 28\n",
      "3 3\n",
      "39 37\n",
      "Saved 6_251\n",
      "19 17\n",
      "18 17\n",
      "1 1\n",
      "42 40\n",
      "Saved 6_252\n",
      "2 2\n",
      "42 40\n",
      "Saved 6_253\n",
      "8 7\n",
      "32 32\n",
      "0 0\n",
      "40 38\n",
      "Saved 6_254\n",
      "Saved 6_255\n",
      "Saved 6_256\n",
      "Saved 6_257\n",
      "Saved 6_258\n",
      "Saved 6_259\n",
      "Saved 6_260\n",
      "Saved 6_261\n",
      "Saved 6_262\n",
      "Saved 6_263\n",
      "Saved 6_264\n",
      "Saved 6_265\n",
      "Saved 6_266\n",
      "Saved 6_267\n",
      "Saved 6_268\n",
      "Saved 6_269\n",
      "Saved 6_270\n",
      "Saved 6_271\n",
      "Saved 6_272\n",
      "Saved 6_273\n",
      "Saved 6_274\n",
      "Saved 6_275\n",
      "Saved 6_276\n",
      "Saved 6_277\n",
      "Saved 6_278\n",
      "Saved 6_279\n",
      "Saved 6_280\n",
      "Saved 6_281\n",
      "Saved 6_282\n",
      "Saved 6_283\n",
      "Saved 6_284\n",
      "Saved 6_285\n",
      "Saved 6_286\n",
      "Saved 6_287\n",
      "Saved 6_288\n",
      "Saved 6_289\n",
      "Saved 6_290\n",
      "Saved 6_291\n",
      "Saved 6_292\n",
      "Saved 6_293\n",
      "Saved 6_294\n",
      "Saved 6_295\n",
      "Saved 6_296\n",
      "Saved 6_297\n",
      "Saved 6_298\n",
      "Saved 6_299\n",
      "Saved 6_300\n",
      "Saved 6_301\n",
      "Saved 6_302\n",
      "Saved 6_303\n",
      "Saved 6_304\n",
      "Saved 6_305\n",
      "Saved 6_306\n",
      "Saved 6_307\n",
      "Saved 6_308\n",
      "Saved 6_309\n",
      "Saved 6_310\n",
      "Saved 6_311\n",
      "Saved 6_312\n",
      "Saved 6_313\n",
      "Saved 6_314\n",
      "Saved 6_315\n",
      "Saved 6_316\n",
      "Saved 6_317\n",
      "Saved 6_318\n",
      "Saved 6_319\n",
      "Saved 6_320\n",
      "Saved 6_321\n",
      "Saved 6_322\n",
      "Saved 6_323\n",
      "Saved 6_324\n",
      "Saved 6_325\n",
      "Saved 6_326\n",
      "Saved 6_327\n",
      "Saved 6_328\n",
      "Saved 6_329\n",
      "Saved 6_330\n",
      "Saved 6_331\n",
      "Saved 6_332\n",
      "Saved 6_333\n",
      "Saved 6_334\n",
      "Saved 6_335\n",
      "Saved 6_336\n",
      "Saved 6_337\n",
      "Saved 6_338\n",
      "Saved 6_339\n",
      "Saved 6_340\n",
      "Saved 6_341\n",
      "Saved 6_342\n",
      "Saved 6_343\n",
      "Saved 6_344\n",
      "Saved 6_345\n",
      "Saved 6_346\n",
      "Saved 6_347\n",
      "Saved 6_348\n",
      "Saved 6_349\n",
      "Saved 6_350\n",
      "Saved 6_351\n",
      "Saved 6_352\n",
      "Saved 6_353\n",
      "Saved 6_354\n",
      "Saved 6_355\n",
      "Saved 6_356\n",
      "Saved 6_357\n",
      "Saved 6_358\n",
      "Saved 6_359\n",
      "Saved 6_360\n",
      "Saved 6_361\n",
      "Saved 6_362\n",
      "Saved 6_363\n",
      "Saved 6_364\n",
      "Saved 6_365\n",
      "Saved 6_366\n",
      "Saved 6_367\n",
      "Saved 6_368\n",
      "Saved 6_369\n",
      "Saved 6_370\n",
      "Saved 6_371\n",
      "Saved 6_372\n",
      "Saved 6_373\n",
      "Saved 6_374\n",
      "Saved 6_375\n",
      "Saved 6_376\n",
      "Saved 6_377\n",
      "Saved 6_378\n",
      "Saved 6_379\n",
      "Saved 6_380\n",
      "Saved 6_381\n",
      "Saved 6_382\n",
      "Saved 6_383\n",
      "Saved 6_384\n",
      "Saved 6_385\n",
      "Saved 6_386\n",
      "Saved 6_387\n",
      "Saved 6_388\n",
      "Saved 6_389\n",
      "Saved 6_390\n",
      "Saved 6_391\n",
      "Saved 6_392\n",
      "Saved 6_393\n",
      "Saved 6_394\n",
      "Saved 6_395\n",
      "Saved 6_396\n",
      "Saved 6_397\n",
      "Saved 6_398\n",
      "Saved 6_399\n",
      "Saved 6_400\n",
      "Saved 6_401\n",
      "Saved 6_402\n",
      "Saved 6_403\n",
      "Saved 6_404\n",
      "Saved 6_405\n",
      "Saved 6_406\n",
      "Saved 6_407\n",
      "Saved 6_408\n",
      "Saved 6_409\n",
      "Saved 6_410\n",
      "Saved 6_411\n",
      "Saved 6_412\n",
      "Saved 6_413\n",
      "Saved 6_414\n",
      "Saved 6_415\n",
      "Saved 6_416\n",
      "Saved 6_417\n",
      "Saved 6_418\n",
      "Saved 6_419\n",
      "Saved 6_420\n",
      "Saved 6_421\n",
      "Saved 6_422\n",
      "Saved 6_423\n",
      "Saved 6_424\n",
      "Saved 6_425\n",
      "Saved 6_426\n",
      "Saved 6_427\n",
      "Saved 6_428\n",
      "Saved 6_429\n",
      "Saved 6_430\n",
      "Saved 6_431\n",
      "Saved 6_432\n",
      "Saved 6_433\n",
      "Saved 6_434\n",
      "Saved 6_435\n",
      "Saved 6_436\n",
      "Saved 6_437\n",
      "Saved 6_438\n",
      "Saved 6_439\n",
      "Saved 6_440\n",
      "Saved 6_441\n",
      "Saved 6_442\n",
      "Saved 6_443\n",
      "Saved 6_444\n",
      "Saved 6_445\n",
      "Saved 6_446\n",
      "Saved 6_447\n",
      "Saved 6_448\n",
      "Saved 6_449\n",
      "Saved 6_450\n",
      "Saved 6_451\n",
      "Saved 6_452\n",
      "Saved 6_453\n",
      "Saved 6_454\n",
      "Saved 6_455\n",
      "Saved 6_456\n",
      "Saved 6_457\n",
      "Saved 6_458\n",
      "Saved 6_459\n",
      "Saved 6_460\n",
      "Saved 6_461\n",
      "Saved 6_462\n",
      "Saved 6_463\n",
      "Saved 6_464\n",
      "Saved 6_465\n",
      "Saved 6_466\n",
      "Saved 6_467\n",
      "Saved 6_468\n",
      "Saved 6_469\n",
      "Saved 6_470\n",
      "Saved 6_471\n",
      "Saved 6_472\n",
      "Saved 6_473\n",
      "Saved 6_474\n",
      "Saved 6_475\n",
      "Saved 6_476\n",
      "Saved 6_477\n",
      "Saved 6_478\n",
      "Saved 6_479\n",
      "Saved 6_480\n",
      "Saved 6_481\n",
      "Saved 6_482\n",
      "Saved 6_483\n",
      "Saved 6_484\n",
      "Saved 6_485\n",
      "Saved 6_486\n",
      "Saved 6_487\n",
      "Saved 6_488\n",
      "Saved 6_489\n",
      "Saved 6_490\n",
      "Saved 6_491\n",
      "Saved 6_492\n",
      "Saved 6_493\n",
      "Saved 6_494\n",
      "Saved 6_495\n",
      "Saved 6_496\n",
      "Saved 6_497\n",
      "Saved 6_498\n",
      "Saved 6_499\n",
      "Saved 6_500\n",
      "Saved 6_501\n",
      "Saved 6_502\n",
      "Saved 6_503\n",
      "Saved 6_504\n",
      "Saved 6_505\n",
      "Saved 6_506\n",
      "Saved 6_507\n",
      "Saved 6_508\n",
      "Saved 6_509\n",
      "Saved 6_510\n",
      "Saved 6_511\n",
      "Saved 6_512\n",
      "Saved 6_513\n",
      "Saved 6_514\n",
      "Saved 6_515\n",
      "Saved 6_516\n",
      "Saved 6_517\n",
      "Saved 6_518\n",
      "Saved 6_519\n",
      "A_MATCH 5329\n",
      "A_NONMATCH 18619\n",
      "A_SAMPLES 23952\n",
      "B_MATCH 5866\n",
      "B_NONMATCH 18433\n",
      "B_SAMPLES 23887\n"
     ]
    }
   ],
   "source": [
    "saveEventData2(data_dir, eventNames, fileLabel, rm_groups = ['A_S|M', 'A_S|NM', 'B_S|M', 'B_S|NM'])  # ['A_SAMPLES', 'B_SAMPLES'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec472928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "19d4adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 519\n",
      "1 / 519\n",
      "2 / 519\n",
      "3 / 519\n",
      "4 / 519\n",
      "5 / 519\n",
      "6 / 519\n",
      "7 / 519\n",
      "8 / 519\n",
      "9 / 519\n",
      "10 / 519\n",
      "11 / 519\n",
      "12 / 519\n",
      "13 / 519\n",
      "14 / 519\n",
      "15 / 519\n",
      "16 / 519\n",
      "17 / 519\n",
      "18 / 519\n",
      "19 / 519\n",
      "20 / 519\n",
      "21 / 519\n",
      "22 / 519\n",
      "23 / 519\n",
      "24 / 519\n",
      "25 / 519\n",
      "26 / 519\n",
      "27 / 519\n",
      "28 / 519\n",
      "29 / 519\n",
      "30 / 519\n",
      "31 / 519\n",
      "32 / 519\n",
      "33 / 519\n",
      "34 / 519\n",
      "35 / 519\n",
      "36 / 519\n",
      "37 / 519\n",
      "38 / 519\n",
      "39 / 519\n",
      "40 / 519\n",
      "41 / 519\n",
      "42 / 519\n",
      "43 / 519\n",
      "44 / 519\n",
      "45 / 519\n",
      "46 / 519\n",
      "47 / 519\n",
      "48 / 519\n",
      "49 / 519\n",
      "50 / 519\n",
      "51 / 519\n",
      "52 / 519\n",
      "53 / 519\n",
      "54 / 519\n",
      "55 / 519\n",
      "56 / 519\n",
      "57 / 519\n",
      "58 / 519\n",
      "59 / 519\n",
      "60 / 519\n",
      "61 / 519\n",
      "62 / 519\n",
      "63 / 519\n",
      "64 / 519\n",
      "65 / 519\n",
      "66 / 519\n",
      "67 / 519\n",
      "68 / 519\n",
      "69 / 519\n",
      "70 / 519\n",
      "71 / 519\n",
      "72 / 519\n",
      "73 / 519\n",
      "74 / 519\n",
      "75 / 519\n",
      "76 / 519\n",
      "77 / 519\n",
      "78 / 519\n",
      "79 / 519\n",
      "80 / 519\n",
      "81 / 519\n",
      "82 / 519\n",
      "83 / 519\n",
      "84 / 519\n",
      "85 / 519\n",
      "86 / 519\n",
      "87 / 519\n",
      "88 / 519\n",
      "89 / 519\n",
      "90 / 519\n",
      "91 / 519\n",
      "92 / 519\n",
      "93 / 519\n",
      "94 / 519\n",
      "95 / 519\n",
      "96 / 519\n",
      "97 / 519\n",
      "98 / 519\n",
      "99 / 519\n",
      "100 / 519\n",
      "101 / 519\n",
      "102 / 519\n",
      "103 / 519\n",
      "104 / 519\n",
      "105 / 519\n",
      "106 / 519\n",
      "107 / 519\n",
      "108 / 519\n",
      "109 / 519\n",
      "110 / 519\n",
      "111 / 519\n",
      "112 / 519\n",
      "113 / 519\n",
      "114 / 519\n",
      "115 / 519\n",
      "116 / 519\n",
      "117 / 519\n",
      "118 / 519\n",
      "119 / 519\n",
      "120 / 519\n",
      "121 / 519\n",
      "122 / 519\n",
      "123 / 519\n",
      "124 / 519\n",
      "125 / 519\n",
      "126 / 519\n",
      "127 / 519\n",
      "128 / 519\n",
      "129 / 519\n",
      "130 / 519\n",
      "131 / 519\n",
      "132 / 519\n",
      "133 / 519\n",
      "134 / 519\n",
      "135 / 519\n",
      "136 / 519\n",
      "137 / 519\n",
      "138 / 519\n",
      "139 / 519\n",
      "140 / 519\n",
      "141 / 519\n",
      "142 / 519\n",
      "143 / 519\n",
      "144 / 519\n",
      "145 / 519\n",
      "146 / 519\n",
      "147 / 519\n",
      "148 / 519\n",
      "149 / 519\n",
      "150 / 519\n",
      "151 / 519\n",
      "152 / 519\n",
      "153 / 519\n",
      "154 / 519\n",
      "155 / 519\n",
      "156 / 519\n",
      "157 / 519\n",
      "158 / 519\n",
      "159 / 519\n",
      "160 / 519\n",
      "161 / 519\n",
      "162 / 519\n",
      "163 / 519\n",
      "164 / 519\n",
      "165 / 519\n",
      "166 / 519\n",
      "167 / 519\n",
      "168 / 519\n",
      "169 / 519\n",
      "170 / 519\n",
      "171 / 519\n",
      "172 / 519\n",
      "173 / 519\n",
      "174 / 519\n",
      "175 / 519\n",
      "176 / 519\n",
      "177 / 519\n",
      "178 / 519\n",
      "179 / 519\n",
      "180 / 519\n",
      "181 / 519\n",
      "182 / 519\n",
      "183 / 519\n",
      "184 / 519\n",
      "185 / 519\n",
      "186 / 519\n",
      "187 / 519\n",
      "188 / 519\n",
      "189 / 519\n",
      "190 / 519\n",
      "191 / 519\n",
      "192 / 519\n",
      "193 / 519\n",
      "194 / 519\n",
      "195 / 519\n",
      "196 / 519\n",
      "197 / 519\n",
      "198 / 519\n",
      "199 / 519\n",
      "200 / 519\n",
      "201 / 519\n",
      "202 / 519\n",
      "203 / 519\n",
      "204 / 519\n",
      "205 / 519\n",
      "206 / 519\n",
      "207 / 519\n",
      "208 / 519\n",
      "209 / 519\n",
      "210 / 519\n",
      "211 / 519\n",
      "212 / 519\n",
      "213 / 519\n",
      "214 / 519\n",
      "215 / 519\n",
      "216 / 519\n",
      "217 / 519\n",
      "218 / 519\n",
      "219 / 519\n",
      "220 / 519\n",
      "221 / 519\n",
      "222 / 519\n",
      "223 / 519\n",
      "224 / 519\n",
      "225 / 519\n",
      "226 / 519\n",
      "227 / 519\n",
      "228 / 519\n",
      "229 / 519\n",
      "230 / 519\n",
      "231 / 519\n",
      "232 / 519\n",
      "233 / 519\n",
      "234 / 519\n",
      "235 / 519\n",
      "236 / 519\n",
      "237 / 519\n",
      "238 / 519\n",
      "239 / 519\n",
      "240 / 519\n",
      "241 / 519\n",
      "242 / 519\n",
      "243 / 519\n",
      "244 / 519\n",
      "245 / 519\n",
      "246 / 519\n",
      "247 / 519\n",
      "248 / 519\n",
      "249 / 519\n",
      "250 / 519\n",
      "251 / 519\n",
      "252 / 519\n",
      "253 / 519\n",
      "254 / 519\n",
      "255 / 519\n",
      "256 / 519\n",
      "257 / 519\n",
      "258 / 519\n",
      "259 / 519\n",
      "260 / 519\n",
      "261 / 519\n",
      "262 / 519\n",
      "263 / 519\n",
      "264 / 519\n",
      "265 / 519\n",
      "266 / 519\n",
      "267 / 519\n",
      "268 / 519\n",
      "269 / 519\n",
      "270 / 519\n",
      "271 / 519\n",
      "272 / 519\n",
      "273 / 519\n",
      "274 / 519\n",
      "275 / 519\n",
      "276 / 519\n",
      "277 / 519\n",
      "278 / 519\n",
      "279 / 519\n",
      "280 / 519\n",
      "281 / 519\n",
      "282 / 519\n",
      "283 / 519\n",
      "284 / 519\n",
      "285 / 519\n",
      "286 / 519\n",
      "287 / 519\n",
      "288 / 519\n",
      "289 / 519\n",
      "290 / 519\n",
      "291 / 519\n",
      "292 / 519\n",
      "293 / 519\n",
      "294 / 519\n",
      "295 / 519\n",
      "296 / 519\n",
      "297 / 519\n",
      "298 / 519\n",
      "299 / 519\n",
      "300 / 519\n",
      "301 / 519\n",
      "302 / 519\n",
      "303 / 519\n",
      "304 / 519\n",
      "305 / 519\n",
      "306 / 519\n",
      "307 / 519\n",
      "308 / 519\n",
      "309 / 519\n",
      "310 / 519\n",
      "311 / 519\n",
      "312 / 519\n",
      "313 / 519\n",
      "314 / 519\n",
      "315 / 519\n",
      "316 / 519\n",
      "317 / 519\n",
      "318 / 519\n",
      "319 / 519\n",
      "320 / 519\n",
      "321 / 519\n",
      "322 / 519\n",
      "323 / 519\n",
      "324 / 519\n",
      "325 / 519\n",
      "326 / 519\n",
      "327 / 519\n",
      "328 / 519\n",
      "329 / 519\n",
      "330 / 519\n",
      "331 / 519\n",
      "332 / 519\n",
      "333 / 519\n",
      "334 / 519\n",
      "335 / 519\n",
      "336 / 519\n",
      "337 / 519\n",
      "338 / 519\n",
      "339 / 519\n",
      "340 / 519\n",
      "341 / 519\n",
      "342 / 519\n",
      "343 / 519\n",
      "344 / 519\n",
      "345 / 519\n",
      "346 / 519\n",
      "347 / 519\n",
      "348 / 519\n",
      "349 / 519\n",
      "350 / 519\n",
      "351 / 519\n",
      "352 / 519\n",
      "353 / 519\n",
      "354 / 519\n",
      "355 / 519\n",
      "356 / 519\n",
      "357 / 519\n",
      "358 / 519\n",
      "359 / 519\n",
      "360 / 519\n",
      "361 / 519\n",
      "362 / 519\n",
      "363 / 519\n",
      "364 / 519\n",
      "365 / 519\n",
      "366 / 519\n",
      "367 / 519\n",
      "368 / 519\n",
      "369 / 519\n",
      "370 / 519\n",
      "371 / 519\n",
      "372 / 519\n",
      "373 / 519\n",
      "374 / 519\n",
      "375 / 519\n",
      "376 / 519\n",
      "377 / 519\n",
      "378 / 519\n",
      "379 / 519\n",
      "380 / 519\n",
      "381 / 519\n",
      "382 / 519\n",
      "383 / 519\n",
      "384 / 519\n",
      "385 / 519\n",
      "386 / 519\n",
      "387 / 519\n",
      "388 / 519\n",
      "389 / 519\n",
      "390 / 519\n",
      "391 / 519\n",
      "392 / 519\n",
      "393 / 519\n",
      "394 / 519\n",
      "395 / 519\n",
      "396 / 519\n",
      "397 / 519\n",
      "398 / 519\n",
      "399 / 519\n",
      "400 / 519\n",
      "401 / 519\n",
      "402 / 519\n",
      "403 / 519\n",
      "404 / 519\n",
      "405 / 519\n",
      "406 / 519\n",
      "407 / 519\n",
      "408 / 519\n",
      "409 / 519\n",
      "410 / 519\n",
      "411 / 519\n",
      "412 / 519\n",
      "413 / 519\n",
      "414 / 519\n",
      "415 / 519\n",
      "416 / 519\n",
      "417 / 519\n",
      "418 / 519\n",
      "419 / 519\n",
      "420 / 519\n",
      "421 / 519\n",
      "422 / 519\n",
      "423 / 519\n",
      "424 / 519\n",
      "425 / 519\n",
      "426 / 519\n",
      "427 / 519\n",
      "428 / 519\n",
      "429 / 519\n",
      "430 / 519\n",
      "431 / 519\n",
      "432 / 519\n",
      "433 / 519\n",
      "434 / 519\n",
      "435 / 519\n",
      "436 / 519\n",
      "437 / 519\n",
      "438 / 519\n",
      "439 / 519\n",
      "440 / 519\n",
      "441 / 519\n",
      "442 / 519\n",
      "443 / 519\n",
      "444 / 519\n",
      "445 / 519\n",
      "446 / 519\n",
      "447 / 519\n",
      "448 / 519\n",
      "449 / 519\n",
      "450 / 519\n",
      "451 / 519\n",
      "452 / 519\n",
      "453 / 519\n",
      "454 / 519\n",
      "455 / 519\n",
      "456 / 519\n",
      "457 / 519\n",
      "458 / 519\n",
      "459 / 519\n",
      "460 / 519\n",
      "461 / 519\n",
      "462 / 519\n",
      "463 / 519\n",
      "464 / 519\n",
      "465 / 519\n",
      "466 / 519\n",
      "467 / 519\n",
      "468 / 519\n",
      "469 / 519\n",
      "470 / 519\n",
      "471 / 519\n",
      "472 / 519\n",
      "473 / 519\n",
      "474 / 519\n",
      "475 / 519\n",
      "476 / 519\n",
      "477 / 519\n",
      "478 / 519\n",
      "479 / 519\n",
      "480 / 519\n",
      "481 / 519\n",
      "482 / 519\n",
      "483 / 519\n",
      "484 / 519\n",
      "485 / 519\n",
      "486 / 519\n",
      "487 / 519\n",
      "488 / 519\n",
      "489 / 519\n",
      "490 / 519\n",
      "491 / 519\n",
      "492 / 519\n",
      "493 / 519\n",
      "494 / 519\n",
      "495 / 519\n",
      "496 / 519\n",
      "497 / 519\n",
      "498 / 519\n",
      "499 / 519\n",
      "500 / 519\n",
      "501 / 519\n",
      "502 / 519\n",
      "503 / 519\n",
      "504 / 519\n",
      "505 / 519\n",
      "506 / 519\n",
      "507 / 519\n",
      "508 / 519\n",
      "509 / 519\n",
      "510 / 519\n",
      "511 / 519\n",
      "512 / 519\n",
      "513 / 519\n",
      "514 / 519\n",
      "515 / 519\n",
      "516 / 519\n",
      "517 / 519\n",
      "518 / 519\n"
     ]
    }
   ],
   "source": [
    "fileLabel = '6' # 'S|M & S|NM(NS)'\n",
    "combineEventData(data_dir, eventNames, fileLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638915f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c130f07b",
   "metadata": {},
   "source": [
    "## Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cfcc3cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95786, 152, 100)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all = loadnpz(data_dir+'/eventData/combined/data_6.npz') # S|M & S|NM(NS).npz')\n",
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9b09fe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95786,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_all = loadnpz(data_dir+'/eventData/combined/outputType_6.npz') # _S|M & S|NM(NS).npz')\n",
    "output_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "da05620a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(output_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fb6606fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_MATCH 5321\n",
      "A_NONMATCH 18571\n",
      "A_SAMPLES 23896\n",
      "B_MATCH 5850\n",
      "B_NONMATCH 18355\n",
      "B_SAMPLES 23793\n",
      "A_MATCH + A_NONMATCH:  23892\n",
      "B_MATCH + B_NONMATCH:  29746\n"
     ]
    }
   ],
   "source": [
    "a, c = np.unique(output_all, return_counts=True)\n",
    "eventNames = loadnpz(data_dir+'/eventData/seperate/outputNames_6.npz') #_S|M & S|NM(NS).npz')\n",
    "# ['A_MATCH', 'A_NONMATCH', 'B_MATCH', 'B_NONMATCH', 'A_S|M', 'A_S|NM', 'B_S|M', 'B_S|NM'] \n",
    "\n",
    "for i in np.arange(len(a)):\n",
    "    print(eventNames[i], c[i])\n",
    "\n",
    "print(\"A_MATCH + A_NONMATCH: \", c[0] + c[1])\n",
    "print(\"B_MATCH + B_NONMATCH: \",c[2] + c[3])\n",
    "# print(\"A diff: \", (c[0] + c[1]) - (c[4] + c[5])) # 23719\n",
    "# print(\"B_diff: \", (c[2] + c[3]) - (c[6] + c[7])) # 23914\n",
    "\n",
    "# some Sample events are followed by another Sample events (Not always in order of Sample & Match/ Nonmatch events)\n",
    "# thus the difference in extracted Sample events given different conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c458612b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 518, 518, 518])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_all = loadnpz(data_dir+'/eventData/combined/keys_S|M & S|NM(NS).npz')\n",
    "key_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3d58ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(519, 152)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputLocation = loadnpz(data_dir+'/eventData/combined/inputLocation_S|M & S|NM(NS).npz')\n",
    "inputLocation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba564484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19a1d034",
   "metadata": {},
   "source": [
    "## Unused Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eef6e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigateEventData(data_dir, eventNames, fileLabel):\n",
    "    \"\"\"\n",
    "    This function saves *******\n",
    "    \"\"\"\n",
    "    \n",
    "    eventNames = ['A_MATCH', 'A_NONMATCH', 'B_MATCH', 'B_NONMATCH', 'A_SAMPLES', 'B_SAMPLES', 'NOSEPOKE']\n",
    "    \n",
    "    #\"validArgs\" will be the set of sessions index with valid data that are saved from the function saveEventData().\n",
    "    validArgs = loadnpz(data_dir+'/eventData/seperate/validArgs_' + fileLabel + '.npz') \n",
    "\n",
    "    #full directory of each session\n",
    "    fileNames = loadFileNames(data_dir)\n",
    "\n",
    "    # loop over all the sessions \n",
    "    for a0 in range(len(validArgs)): \n",
    "        a = validArgs[a0]\n",
    "        fileName = fileNames[a] \n",
    "        reader = nexfile.Reader()\n",
    "        fileData = reader.ReadNexFile(fileName)\n",
    "\n",
    "        #This gets the names of all the variables which have timestamps data\n",
    "        variableNames = []\n",
    "        for b in range(len(fileData['Variables'])):\n",
    "            if 'Timestamps' in fileData['Variables'][b].keys():\n",
    "                varName = fileData['Variables'][b]['Header'][\"Name\"]\n",
    "                variableNames.append(varName)\n",
    "#             else: \n",
    "#                 print(\"b\", b)\n",
    "#                 print(\"no timestamp\", fileData['Variables'][b]['Header'][\"Name\"])\n",
    "                \n",
    "        variableNames = np.array(variableNames)\n",
    "\n",
    "#         print (len(fileData['Variables']))\n",
    "#         print(\"1size\", len(variableNames))\n",
    "        \n",
    "        #This gets the neuron spike data and the event time data.\n",
    "        variableNames2 = []\n",
    "        inputNums = []\n",
    "        inputNames = []\n",
    "        outputNums = []\n",
    "        outputNames = []\n",
    "        spikeData = []\n",
    "        b = 0\n",
    "        for b0 in range(len(fileData['Variables'])):\n",
    "            if 'Timestamps' in fileData['Variables'][b].keys():       \n",
    "                #This gives the timing data for the variable\n",
    "                spikes = fileData['Variables'][b]['Timestamps']\n",
    "                spikes = np.array(spikes)\n",
    "                spikeData.append(np.copy(spikes))\n",
    "\n",
    "                #This puts the variable number in \"spikeData\" for each relevent variable\n",
    "                name = variableNames[b]\n",
    "                variableNames2.append(name)\n",
    "                \n",
    "                #This appends the arg in \"fileData['Variables']\" which corresponds to the variable \"name\"\n",
    "                if ('wire' in name) and ('cell' in name):\n",
    "                    inputNums.append(b)\n",
    "                    inputNames.append(name)\n",
    "                if name in eventNames:\n",
    "                    outputNums.append(b)\n",
    "                    outputNames.append(name)\n",
    "                b += 1\n",
    "#             else:\n",
    "#                 print(\"b0\", b0)\n",
    "#                 print(\"no timestamp\", fileData['Variables'][b0]['Header'][\"Name\"])\n",
    "\n",
    "        variableNames2 = np.array(variableNames2)\n",
    "        \n",
    "#         print (\"2\", variableNames2)\n",
    "#         print(\"2size\", len(variableNames2))\n",
    "\n",
    "        #ALL_S_PHASE: A_SAMPLES, B_SAMPLES\n",
    "        #NOSEPOKE\n",
    "        #TRIAL\n",
    "        existSpike = np.concatenate((spikeData[outputNums[0]] , spikeData[outputNums[1]] , spikeData[outputNums[2]] , spikeData[outputNums[3]]))\n",
    "#         if False:\n",
    "#             for b in range(len(variableNames2)):\n",
    "#                 if not 'cell' in variableNames2[b]:\n",
    "#                     print (variableNames2[b])\n",
    "#                     spike1 = spikeData[b]\n",
    "#                     print (spike1.shape)\n",
    "#                     print (np.intersect1d(spike1, existSpike).shape)\n",
    "\n",
    "        #quit()\n",
    "\n",
    "        #arg1 = np.argwhere(variableNames2 == 'NOSEPOKE')[0, 0] #NOSEPOKE, REWARDCOUNT\n",
    "        #arg2 = np.argwhere(variableNames2 == 'TRIAL')[0, 0]\n",
    "        #spike1 = spikeData[arg1]\n",
    "        #spike2 = spikeData[arg2]\n",
    "\n",
    "        #print (spike1.shape)\n",
    "        #print (spike2.shape)\n",
    "        #print (np.intersect1d(spike1, spike2).shape)\n",
    "        #print (variableNames2)\n",
    "        #quit()\n",
    "\n",
    "\n",
    "        outputNums = np.array(outputNums)[np.argsort(np.array(outputNames))]\n",
    "\n",
    "        numOutput = 0\n",
    "        outputType = np.array([])\n",
    "        for b0 in range(len(outputNums)):\n",
    "            b = outputNums[b0]\n",
    "            spikes0 = spikeData[b]\n",
    "            numOutput += spikes0.shape[0]\n",
    "\n",
    "            b1 = np.argwhere(eventNames == outputNames[b0])[0, 0]\n",
    "\n",
    "            outputType = np.concatenate(( outputType, np.zeros(spikes0.shape) + b1  ))\n",
    "\n",
    "\n",
    "        spikeTimer = np.zeros((numOutput, len(inputNums), 2000 ))\n",
    "\n",
    "        b0 = 0\n",
    "        for b in outputNums:\n",
    "            spikes0 = spikeData[b]\n",
    "            for c in range(spikes0.shape[0]):\n",
    "                timeNow = spikes0[c]\n",
    "\n",
    "                d0 = 0\n",
    "                for d in inputNums:\n",
    "                    spikes = spikeData[d] - timeNow\n",
    "                    spikes = spikes[np.abs(spikes) < 2]\n",
    "                    spikes = spikes + 2\n",
    "                    spikes = np.floor(spikes * 500).astype(int)\n",
    "\n",
    "                    spikeTimer[b0, d0, spikes] = 1\n",
    "\n",
    "                    #print (timeNow)\n",
    "                    #print (spikes)\n",
    "                    #quit()\n",
    "                    d0 += 1\n",
    "                #print (len(spikes))\n",
    "                #quit()\n",
    "\n",
    "                b0 += 1\n",
    "\n",
    "        print (np.unique(outputType))\n",
    "\n",
    "\n",
    "#         plt.plot(np.sum(np.sum(spikeTimer, axis=1), axis=1))\n",
    "#         plt.show()\n",
    "\n",
    "        np.savez_compressed(data_dir + '/eventData/seperate/investigate/data_' + fileLabel + '_' + str(a) + '.npz', spikeTimer)\n",
    "        inputNames = np.array(inputNames)\n",
    "        np.savez_compressed(data_dir + '/eventData/seperate/investigate/input_' + fileLabel + '_' + str(a) + '.npz', inputNames)\n",
    "        np.savez_compressed(data_dir + '/eventData/seperate/investigate/output_' + fileLabel + '_' + str(a) + '.npz', outputType)\n",
    "\n",
    "    validArgs = np.array(validArgs)\n",
    "    np.savez_compressed(data_dir + '/eventData/seperate/investigate/validArgs_' + fileLabel + '.npz', validArgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9fec0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLocationNum(data_dir):\n",
    "\n",
    "    #This loads the list of all neuron channel names\n",
    "    inputNamesUnique = loadnpz(data_dir + '/eventData/general/uniqueNames.npz')\n",
    "\n",
    "    locNames = [] #This list will include the brain location for all input neuron channels\n",
    "    hemNames = [] #This list will include the hemisphere for all input neuron channels\n",
    "    for a in range(inputNamesUnique.shape[0]):\n",
    "        name = inputNamesUnique[a]\n",
    "        name = name.split('_')\n",
    "        #name = name[1:-4]\n",
    "\n",
    "\n",
    "        hemName = name[1]\n",
    "        locName = name[2]\n",
    "\n",
    "        locNames.append(locName)\n",
    "        hemNames.append(hemName)\n",
    "\n",
    "\n",
    "    locNames = np.array(locNames)\n",
    "    hemNames = np.array(hemNames)\n",
    "    locNamesUnique, locNames = np.unique(locNames, return_inverse=True) #Converts brain location to number\n",
    "    hemNamesUnique, hemNames = np.unique(hemNames, return_inverse=True) #Converts hemisphere to number\n",
    "\n",
    "    #Saves information\n",
    "    np.savez_compressed(data_dir + '/eventData/general/brainLocationNamesUnique.npz', locNamesUnique)\n",
    "    np.savez_compressed(data_dir + '/eventData/general/brainLocationNames.npz', locNames)\n",
    "    np.savez_compressed(data_dir + '/eventData/general/brainHemisphereNames.npz', hemNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2864bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLocationNum(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7055222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveWireNum(data_dir):\n",
    "\n",
    "    inputNamesUnique = loadnpz(data_dir + '/eventData/general/uniqueNames.npz') #This loads the names of the neuron channels\n",
    "\n",
    "    wireName = [] #This list includes the wire names for all neuron input channels\n",
    "    for a in range(inputNamesUnique.shape[0]):\n",
    "        name = inputNamesUnique[a]\n",
    "        name = name.split('_')\n",
    "        name = name[1:-2]\n",
    "        name = '_'.join(name)\n",
    "        wireName.append(name)\n",
    "\n",
    "    wireName = np.array(wireName)\n",
    "    wireNameUnique, wireName = np.unique(wireName, return_inverse=True) #This converts the wire name to a number\n",
    "\n",
    "\n",
    "    np.savez_compressed(data_dir + '/eventData/general/wireNames.npz', wireName) #This saves the wire number for each neuron input channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18761331",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveWireNum(data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
